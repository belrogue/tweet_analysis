# -*- coding: utf-8 -*-
"""vivek-text-analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PRfRsqThcqLowqQJUOLozfbQFyHKfRky
"""

import nltk
import pdb
from string import punctuation
nltk.download('webtext')
nltk.download('treebank')
nltk.download('gutenberg')
nltk.download('genesis')
nltk.download('inaugural')
nltk.download('nps_chat')
nltk.download('twitter_samples')
nltk.download('stopwords')

from nltk.corpus import twitter_samples

#from nltk.book import *

stopwords = nltk.corpus.stopwords.words('english')
stopwords.append('â€¦')
stopwords.append('rt')
print(stopwords)

def normalization(word_list):
        lem = nltk.WordNetLemmatizer()
        normalized_word = []
        for word in word_list:
            normalized_text = lem.lemmatize(word,'v')
            normalized_word.append(normalized_text)
        return normalized_word

#text1.concordance("monstrous")

#fdist1 = FreqDist(text1)
#fdist1.most_common(50)
#
#allWordExceptStopDist = nltk.FreqDist(w.lower().rstrip(punctuation) for w in text1 if w.lower() not in stopwords)
#
#print(allWordExceptStopDist.most_common(50))
#
#nltk.download('wordnet')
#
#nom_text1 = normalization(text1)
#
#allWordExceptStopDist = nltk.FreqDist(w.lower().rstrip(punctuation) for w in nom_text1 if w.lower() not in stopwords)
#
#print(" ----- ")
#
#print(allWordExceptStopDist.most_common(50))

print(" ----- ")

tokenized = twitter_samples.tokenized('tweets.20150430-223406.json')
for toks in tokenized[:5]:
    print(toks)

input_file = twitter_samples.abspath("tweets.20150430-223406.json")

from nltk.twitter.common import json2csv
with open(input_file) as fp:
    json2csv(fp, 'tweets_text.csv',
            ['created_at', 'favorite_count', 'id', 'in_reply_to_status_id', 
            'in_reply_to_user_id', 'retweet_count', 'retweeted', 
            'text', 'truncated', 'user.id'])

print (" ----- ")
import pandas as pd
tweets = pd.read_csv('tweets_text.csv', index_col=2, header=0, encoding="utf8")
print(tweets.head(5))

print (" ----- ")
#tweets = tweets.loc[tweets['user.id'] == 557422508]['text']
#tweets = tweets.head(300)
# tweets = tweets['user.id']
print(tweets["user.id"].value_counts(ascending = False ))

#pdb.set_trace()

raw_text1 = ""
for tweet in tweets:
    raw_text1 = raw_text1 + str(tweet) + "\n"

from nltk.tokenize import TweetTokenizer
tknzr = TweetTokenizer()
text1 = nltk.Text(tknzr.tokenize(raw_text1))

#pdb.set_trace()
#print("Text1: ")
#print(text1)

allHashtags = nltk.FreqDist(w.lower().rstrip(punctuation) for w in text1 if w[0] == '#')

print (" ---- ")
print(allHashtags.most_common(20))
print (" ---- ")

allWordExceptStopDist = nltk.FreqDist(w.lower().rstrip(punctuation) for w in text1 if w.lower() not in stopwords)

print(allWordExceptStopDist.most_common(50))

nom_text1 = normalization(text1)

allWordExceptStopDist = nltk.FreqDist(w.lower().rstrip(punctuation) for w in nom_text1 if w.lower() not in stopwords and w[0] != '#' and len(w) > 1)

print(" ----- ")

print(allWordExceptStopDist.most_common(50))
print(" Word: ")
print(allWordExceptStopDist.most_common(50)[0][0])
print(" Len: ")
print(len(allWordExceptStopDist.most_common(50)[0][0]))
